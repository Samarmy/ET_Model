{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-3), x.size(-2), x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-3),  y.size(-2),  y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            'conv1',\n",
    "            nn.Conv2d(num_input_features,\n",
    "                      bn_size * growth_rate,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=False))\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            'conv2',\n",
    "            nn.Conv2d(bn_size * growth_rate,\n",
    "                      growth_rate,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False))\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super().forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features,\n",
    "                                     p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate,\n",
    "                                growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer{}'.format(i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super().__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module(\n",
    "            'conv',\n",
    "            nn.Conv2d(num_input_features,\n",
    "                      num_output_features,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"Densenet-BC model class\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_input_channels=11,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 growth_rate=32,\n",
    "                 block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64,\n",
    "                 bn_size=4,\n",
    "                 drop_rate=0,\n",
    "                 num_classes=100):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = [('conv1',\n",
    "                          nn.Conv2d(n_input_channels,\n",
    "                                    num_init_features,\n",
    "                                    kernel_size=4,\n",
    "                                    stride=2,\n",
    "                                    padding=1,\n",
    "                                    bias=False)),\n",
    "                         ('norm1', nn.BatchNorm2d(num_init_features)),\n",
    "                         ('relu1', nn.ReLU(inplace=True))]\n",
    "        if not no_max_pool:\n",
    "            self.features.append(\n",
    "                ('pool1', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)))\n",
    "        self.features = nn.Sequential(OrderedDict(self.features))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                num_input_features=num_features,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock{}'.format(i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition{}'.format(i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out,\n",
    "                                    output_size=(1, 1,\n",
    "                                                 1)).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [121, 169, 201, 264]\n",
    "\n",
    "    if model_depth == 121:\n",
    "        model = DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 24, 16),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 169:\n",
    "        model = DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 32, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 201:\n",
    "        model = DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 48, 32),\n",
    "                         **kwargs)\n",
    "    elif model_depth == 264:\n",
    "        model = DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 64, 48),\n",
    "                         **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseNetGRU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Encoder = TimeDistributed(generate_model(121))\n",
    "        self.RNN = nn.GRU(100, 100, 4)\n",
    "        self.Output = nn.Linear(100, 1)\n",
    "        \n",
    "#     seq_len, batch, input_size\n",
    "    def forward(self, x):\n",
    "        out = self.Encoder(x)\n",
    "        out = self.RNN(out)\n",
    "        out = self.Output(out[0])\n",
    "        return out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarmst/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:124: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "net = DenseNetGRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = torch.randn((7, 4, 11, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = net(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
