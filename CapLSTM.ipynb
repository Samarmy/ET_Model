{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-3), x.size(-2), x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y, reconstruction = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-3),  y.size(-2),  y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "            reconstruction = reconstruction.view(-1, x.size(1), reconstruction.size(-3),  reconstruction.size(-2),  reconstruction.size(-1))\n",
    "        return y, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(s, dim=-1):\n",
    "    '''\n",
    "    \"Squashing\" non-linearity that shrunks short vectors to almost zero length and long vectors to a length slightly below 1\n",
    "    Eq. (1): v_j = ||s_j||^2 / (1 + ||s_j||^2) * s_j / ||s_j||\n",
    "    Args:\n",
    "        s: Vector before activation\n",
    "        dim: Dimension along which to calculate the norm\n",
    "    Returns:\n",
    "        Squashed vector\n",
    "    '''\n",
    "    squared_norm = torch.sum(s**2, dim=dim, keepdim=True)\n",
    "    return squared_norm / (1 + squared_norm) * s / (torch.sqrt(squared_norm) + 1e-8)\n",
    "\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dim_caps,\n",
    "    kernel_size=9, stride=2, padding=0):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "\n",
    "        Args:\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            dim_caps: Dimensionality, i.e. length, of the output capsule vector.\n",
    "        \"\"\"\n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self._caps_channel = int(out_channels / dim_caps)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), self._caps_channel, out.size(2), out.size(3), self.dim_caps)\n",
    "        out = out.view(out.size(0), -1, self.dim_caps)\n",
    "        return squash(out)\n",
    "\n",
    "\n",
    "class RoutingCapsules(nn.Module):\n",
    "    def __init__(self, in_dim, in_caps, num_caps, dim_caps, num_routing, device: torch.device):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "\n",
    "        Args:\n",
    "            in_dim: Dimensionality (i.e. length) of each capsule vector.\n",
    "            in_caps: Number of input capsules if digits layer.\n",
    "            num_caps: Number of capsules in the capsule layer\n",
    "            dim_caps: Dimensionality, i.e. length, of the output capsule vector.\n",
    "            num_routing: Number of iterations during routing algorithm\n",
    "        \"\"\"\n",
    "        super(RoutingCapsules, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.in_caps = in_caps\n",
    "        self.num_caps = num_caps\n",
    "        self.dim_caps = dim_caps\n",
    "        self.num_routing = num_routing\n",
    "        self.device = device\n",
    "\n",
    "        self.W = nn.Parameter( 0.01 * torch.randn(1, num_caps, in_caps, dim_caps, in_dim ) )\n",
    "\n",
    "    def __repr__(self):\n",
    "        tab = '  '\n",
    "        line = '\\n'\n",
    "        next = ' -> '\n",
    "        res = self.__class__.__name__ + '('\n",
    "        res = res + line + tab + '(' + str(0) + '): ' + 'CapsuleLinear('\n",
    "        res = res + str(self.in_dim) + ', ' + str(self.dim_caps) + ')'\n",
    "        res = res + line + tab + '(' + str(1) + '): ' + 'Routing('\n",
    "        res = res + 'num_routing=' + str(self.num_routing) + ')'\n",
    "        res = res + line + ')'\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # (batch_size, in_caps, in_dim) -> (batch_size, 1, in_caps, in_dim, 1)\n",
    "        x = x.unsqueeze(1).unsqueeze(4)\n",
    "        #\n",
    "        # W @ x =\n",
    "        # (1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =\n",
    "        # (batch_size, num_caps, in_caps, dim_caps, 1)\n",
    "        u_hat = torch.matmul(self.W, x)\n",
    "        # (batch_size, num_caps, in_caps, dim_caps)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        # detach u_hat during routing iterations to prevent gradients from flowing\n",
    "        temp_u_hat = u_hat.detach()\n",
    "\n",
    "        '''\n",
    "        Procedure 1: Routing algorithm\n",
    "        '''\n",
    "        b = torch.zeros(batch_size, self.num_caps, self.in_caps, 1).to(self.device)\n",
    "\n",
    "        for route_iter in range(self.num_routing-1):\n",
    "            # (batch_size, num_caps, in_caps, 1) -> Softmax along num_caps\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # element-wise multiplication\n",
    "            # (batch_size, num_caps, in_caps, 1) * (batch_size, in_caps, num_caps, dim_caps) ->\n",
    "            # (batch_size, num_caps, in_caps, dim_caps) sum across in_caps ->\n",
    "            # (batch_size, num_caps, dim_caps)\n",
    "            s = (c * temp_u_hat).sum(dim=2)\n",
    "            # apply \"squashing\" non-linearity along dim_caps\n",
    "            v = squash(s)\n",
    "            # dot product agreement between the current output vj and the prediction uj|i\n",
    "            # (batch_size, num_caps, in_caps, dim_caps) @ (batch_size, num_caps, dim_caps, 1)\n",
    "            # -> (batch_size, num_caps, in_caps, 1)\n",
    "            uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
    "            b += uv\n",
    "\n",
    "        # last iteration is done on the original u_hat, without the routing weights update\n",
    "        c = F.softmax(b, dim=1)\n",
    "        s = (c * u_hat).sum(dim=2)\n",
    "        # apply \"squashing\" non-linearity along dim_caps\n",
    "        v = squash(s)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleNetwork(nn.Module):\n",
    "    def __init__(self, img_shape, channels, primary_dim, num_classes, out_dim, num_routing, device: torch.device, kernel_size=9):\n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "\n",
    "        self.conv1 = nn.Conv2d(img_shape[0], channels, kernel_size, stride=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.primary = PrimaryCapsules(channels, channels, primary_dim, kernel_size)\n",
    "        \n",
    "        primary_caps = int(channels / primary_dim * ( img_shape[1] - 2*(kernel_size-1) ) * ( img_shape[2] - 2*(kernel_size-1) ) / 4)\n",
    "        self.digits = RoutingCapsules(primary_dim, primary_caps, num_classes, out_dim, num_routing, device=self.device)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(out_dim * num_classes, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, int(prod(img_shape)) )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.primary(out)\n",
    "        out = self.digits(out)\n",
    "        preds = torch.norm(out, dim=-1)\n",
    "\n",
    "        # Reconstruct the *predicted* image\n",
    "        _, max_length_idx = preds.max(dim=1)\t\n",
    "        y = torch.eye(self.num_classes).to(self.device)\n",
    "        y = y.index_select(dim=0, index=max_length_idx).unsqueeze(2)\n",
    "\n",
    "        reconstructions = self.decoder( (out*y).view(out.size(0), -1) )\n",
    "        reconstructions = reconstructions.view(-1, *self.img_shape)\n",
    "\n",
    "        return preds, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Encoder = TimeDistributed(CapsuleNetwork(img_shape=(11, 64, 64), channels=256, primary_dim=8, num_classes=100, out_dim=16, num_routing=3, device=torch.device(\"cpu\")))\n",
    "        self.RNN = nn.LSTM(100, 100, 4)\n",
    "        self.Output = nn.Linear(100, 1)\n",
    "        \n",
    "#     seq_len, batch, input_size\n",
    "    def forward(self, x):\n",
    "        out, reconstruction = self.Encoder(x)\n",
    "        out = self.RNN(out)\n",
    "        out = self.Output(out[0])\n",
    "        return out[-1], reconstruction[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CapLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = torch.randn((2, 2, 11, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output, test_reconstruction = net(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 11, 64, 64]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape, test_reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
